\documentclass[preprint]{aastex} 

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{natbib}
\usepackage{natbibspacing}
\setlength{\bibspacing}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\headsep}{0pt}  
\setlength{\topskip}{0pt}
\setlength{\topmargin}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\footnotesep}{8pt}
\pagestyle{empty}
\citestyle{aa}

\newcommand{\simgt}{\stackrel{>}{_{\sim}}}
\def\kperp{k_{\bot}}
\def\kpar{k_{\|}}
\def\k{{\bf k}}
\def\sky{{\theta}}
\def\HI{{H{\small I }}}
\def\HII{{H{\small II }}}
\def\xHI{{x_{\rm\HI}}}

\def\EnumCompress{\parskip-4pt}

%\usepackage{subfig}
%\usepackage[countmax]{subfloat}

\begin{document}
\title{Data Management Plan}

% Plans for data management and sharing of the products of research. Proposals must include a supplementary document of no more than two pages labeled “Data Management Plan”. This supplement should describe how the proposal will conform to NSF policy on the dissemination and sharing of research results (see AAG Chapter VI.D.4), and may include:
% 
%     the types of data, samples, physical collections, software, curriculum materials, and other materials to be produced in the course of the project;


This proposal will involve the collection and analysis data from the HERA-331 instrument, as well as its design and construction.

We distinguish three distinct locations where data are processed and stored: the Karoo Array Processing Building ({\bf KAPB}) in South Africa, at the University of Pennsylvania ({\bf Penn}) and at the Massachusetts Institute of Technology ({\bf MIT}).

\section{Data Types}

Briefly, the data path is as follows. Raw data generated by the HERA correlator is transferred to a RAID system in the KAPB, and compressed on-site using the correlator X-Engines running during the day.  Compressed data is stored on-site, and is also transferred to Penn via streaming over the Internet, as well as being transferred to small disks which are mailed.  Initially, this will use co-PI Aguirre's cluster and network storage (see Facilities, Equipment, and Other Resources), but will eventually be augmented to a 30-node dedicated cluster plus 250 TB storage for full HERA-331 operation.   Further data analysis and reduction are performed on the Penn cluster, and subsets of the data distributed to collaborators.  Publicly available data are storage at MIT.  The types of data generated, archived, and served are
\EnumCompress
\begin{enumerate}
 \EnumCompress
 \item Raw data
 \item Compressed data
 \item Calibration.  Real time calibration will run on the QA server in the KAPB.
 \item Calibrated snapshot images.  More precise calibration solutions will be stored at Penn.
 \item Searchable database and metadata.  
 \item High dynamic range maps
 \item Calibrated, compressed LST-averaged visibilities
 \item Derived data products
 \EnumCompress
 \begin{enumerate}
 \EnumCompress
  \item Source catalogs
  \item Global sky model
  \item Foreground-subtracted data cubes of high-redshift $21\,\textrm{cm}$ intensity maps and the associated ionization field
  \end{enumerate}
\end{enumerate}

\parskip0pt

A large number of data products will be available.  Imaging products will include coarse real-time calibrated snapshot images in addition to high dynamic range survey maps from the FHD pipeline.  Calibrated, compressed, LST-binned visibilities will also be made public for re-analysis.  Higher cadence data sets will not be hosted publicly in a continuous fashion due to data transfer limitations, but will be available to any users upon request.  A number of derived data products will also be made accessible.  Source catalogs will be provided, and a Global Sky Model of diffuse foregrounds will be updated with new observations and released.  Foreground-subtracted data cubes of high-redshift $21\,\textrm{cm}$ intensity maps and the associated ionization field will be available for cross-correlation studies, as well as to provide context for high redshift optical and IR studies.

Data reduction will use 1) internally generated custom software, 2) the open-source AIPY\footnote{\tt https://pypi.python.org/pypi/aipy} package, 3) publicly available packages (e.g. CASA\footnote{ \tt http://casa.nrao.edu/} and {\sc miriad}).  The data reduction will be described in publication in refereed articles.  

% 
%     the standards to be used for data and metadata format and content (where existing standards are absent or deemed inadequate, this should be documented along with any proposed solutions or remedies);

\section{Data Format and Content Standards}

The Penn cluster will hold a compressed archive in {\sc miriad} format which will be accessible via the Internet 18 months after the data is processed. 

% 
%     policies for access and sharing including provisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements;
% 
%     policies and provisions for re-use, re-distribution, and the production of derivatives; and
%
\section{Data Distribution Policies}

The HERA measurements will be released to the community after an 18-month proprietary period and hosted on a public server at MIT.  Other scientists who may wish to access the data may ask the collaboration for access once the quality of the data is assured.  
%(see \S\ref{sec:DataProducts}).  

%     plans for archiving data, samples, and other research products, and for preservation of access to them.

Final antenna element construction schematics will be made available
for future production of the HERA elements. These designs will be in industry standard format.
Data collected about the electromagnetic properties of the small array will be
disseminated in a scientific publication characterizing in detail the
performance of the HERA elements.

\section{Archiving Plan}

The raw data generated by the correlator is maintained on-site in the KAPB, but under our baseline scenario is not distributed further.  This data is maintained indefinitely using RAID6 storage devices.  
% The calibration, imaging, and data-reduction pipelines associated with
% evaluating the performance of the 7-element hexagonal close-packed array
% will be
% implemented using the open-source AIPY software framework, and will run on the
% computing cluster and data archive for PAPER
% that is maintained at UPenn.  This currently consists of 22 nodes connected by Gb
% ethernet. These nodes consist of 16 Dell PowerEdge 1950s (dual 4-core Intel Xeon L5420 \@ 2.50GHz)
% and 6 Dell PowerEdge R410 (32 GB RAM; dual 6-core Intel Xeon E5649 \@ 2.53GHz),
% for a total of 200 cores and 448 GB RAM.  Fast working data storage is provided
% by a network storage system (NSS) based around two Dell MD1200s, with a total
% RAID storage capacity of 140 TB.
The Penn cluster will host the principle storage of data for the collaboration, and data distribution will
be managed through account access to this system.  All observed data will be stored and archived for the during of this grant at this location.  Resources exist to maintain the archive beyond the grant to honor the  public release of data after the 18-month proprietary period.
% 




\end{document}
